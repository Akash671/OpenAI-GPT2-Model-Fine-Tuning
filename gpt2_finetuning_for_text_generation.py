# -*- coding: utf-8 -*-
"""GPT2_FineTuning_For_Text_Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1im13JySft_6aXLU4FlYiNnf1iyCw4AuI
"""
import json
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
import torch
import os

# Ensure datasets and transformers are installed:
# pip install datasets transformers torch

# --- 1. Load the dataset ---
def load_text_gen_dataset(file_path="fake_text_gen_dataset.jsonl"):
    """
    Loads the fake text generation dataset from a JSONL file.
    """
    data = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                data.append(json.loads(line))
        print(f"Successfully loaded dataset from {file_path}. Found {len(data)} samples.")
        return data
    except FileNotFoundError:
        print(f"Error: Dataset file not found at {file_path}. Please run the data generation script first.")
        return None
    except json.JSONDecodeError as e:
        print(f"Error: Could not decode JSON from a line in {file_path}. Check file format. Error: {e}")
        return None

# Load your dataset
raw_datasets = load_text_gen_dataset()

if raw_datasets is None:
    exit() # Exit if dataset loading failed

hf_dataset = Dataset.from_list(raw_datasets)
train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)
dataset_dict = DatasetDict({
    'train': train_test_split['train'],
    'validation': train_test_split['test']
})
print(f"Dataset split into: {dataset_dict}")


# --- 2. Load pre-trained GPT-2 model and tokenizer ---
# Using 'gpt2' as the base model.
model_checkpoint = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForCausalLM.from_pretrained(model_checkpoint)

print(f"Loaded tokenizer and model: {model_checkpoint}")

# GPT-2 tokenizer does not have a padding token by default.
# For batch processing, a padding token is usually needed.
# We set it to the EOS token for causal language modeling, as suggested by Hugging Face.
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = tokenizer.eos_token_id
    print(f"Set tokenizer.pad_token to tokenizer.eos_token ({tokenizer.eos_token_id})")

# --- 3. Preprocess the dataset ---
# For causal language modeling, we concatenate the prompt and completion.
# The model will then learn to generate the completion given the prompt.
max_length = 256 # Adjust max_length based on your average sequence length

def preprocess_function(examples):
    # Combine prompt and completion. Add EOS token to mark the end of a sequence.
    # The model will learn to generate text until it hits the EOS token.
    texts = [f"{p}{c}{tokenizer.eos_token}" for p, c in zip(examples["prompt"], examples["completion"])]
    return tokenizer(texts, truncation=True, max_length=max_length, padding="max_length")

print("Preprocessing training examples...")
tokenized_datasets = dataset_dict.map(
    preprocess_function,
    batched=True,
    num_proc=os.cpu_count(), # Use multiple processes for faster mapping
    remove_columns=["prompt", "completion"] # Remove original text columns
)
print("Preprocessing complete.")

# Data Collator for Language Modeling will handle batching and masking.
# For Causal Language Modeling (mlm=False), it shifts the labels for next token prediction.
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False # False for Causal Language Modeling
)


# --- 4. Define training arguments ---
output_dir = "./gpt2_fine_tuned_textgen"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch", # Evaluate every epoch
    learning_rate=2e-5,
    per_device_train_batch_size=4, # Adjust based on your GPU memory
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./gpt2_logs",
    logging_steps=10,
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    push_to_hub=False,
    do_train=True,
    do_eval=True,
    # Gradient accumulation and checkpointing can be useful for larger models/smaller GPUs
    # gradient_accumulation_steps=2, # Effectively doubles batch size
    # gradient_checkpointing=True, # Saves memory but slows down training
)

# --- 5. Initialize Trainer ---
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator, # Use the data collator for language modeling
)

# --- 6. Train the model ---
print("Starting GPT-2 model training...")
trainer.train()
print("GPT-2 Training complete!")

# Save the fine-tuned model and tokenizer
model_save_path = "./fine_tuned_gpt2_textgen"
if not os.path.exists(model_save_path):
    os.makedirs(model_save_path)

trainer.save_model(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"Fine-tuned GPT-2 model and tokenizer saved to {model_save_path}")

print("\nYou can now load this model for inference and text generation:")
print(f"""
from transformers import pipeline

# Load the fine-tuned model
generator = pipeline(
    'text-generation',
    model="{model_save_path}",
    tokenizer="{model_save_path}"
)

# Example generation
prompt = "Tech Innovation Boosts Economy (Generated Sample 1)"
generated_text = generator(prompt, max_length=100, num_return_sequences=1,
                           pad_token_id=tokenizer.eos_token_id)[0]['generated_text']
print(f"\\nPrompt: {{prompt}}")
print(f"\\nGenerated: {{generated_text}}")

# You might need to post-process the generated text to remove the prompt itself
# or any unwanted parts like repeated prompts/EOS tokens.
""")

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import os

# Define the path where your fine-tuned GPT-2 model and tokenizer are saved
model_path = "./fine_tuned_gpt2_textgen"

# --- 1. Load the fine-tuned model and tokenizer ---
print(f"Loading fine-tuned GPT-2 model and tokenizer from: {model_path}")
if not os.path.exists(model_path):
    print(f"Error: Model directory not found at {model_path}. "
          "Please ensure the fine-tuning script completed successfully "
          "and saved the model to this location.")
    exit()

try:
    # Load the tokenizer first to set its pad_token if it was set during training
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)

    # Ensure the tokenizer has a padding token defined, typically the EOS token for GPT-2
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.eos_token_id
        print(f"Set tokenizer.pad_token to tokenizer.eos_token ({tokenizer.eos_token_id}) for prediction.")

    print("GPT-2 model and tokenizer loaded successfully!")
except Exception as e:
    print(f"Failed to load GPT-2 model or tokenizer. Error: {e}")
    print("This might happen if the model was not saved correctly or if there's a version mismatch.")
    exit()

# --- 2. Create a text generation pipeline ---
# The pipeline handles tokenization, model inference, and decoding the generated tokens.
# You can specify the device if you have a GPU (e.g., device=0 for the first GPU)
generator = pipeline(
    'text-generation',
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1 # Use GPU if available, else CPU
)

print(f"Text generation pipeline initialized. Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}")

# --- 3. Test with example prompts ---

print("\n--- Generating Text ---")

# Example 1: Use a prompt similar to what was in your training data (a headline)
prompt1 = "Tech Innovation Boosts Economy (Generated Sample 1)"
print(f"\nPrompt: {prompt1}")
# `max_length` controls how long the generated text can be (including the prompt)
# `num_return_sequences` generates multiple distinct outputs
# `pad_token_id` is important for generation with padding
# `do_sample=True` enables sampling, `top_k` and `temperature` add randomness/creativity
generated_text1 = generator(
    prompt1,
    max_length=100, # Total length of prompt + generated completion
    num_return_sequences=1,
    pad_token_id=tokenizer.eos_token_id,
    do_sample=True,
    top_k=50,
    temperature=0.7
)[0]['generated_text']
print(f"Generated Text 1:\n{generated_text1}")
print("-" * 30)


# Example 2: Another prompt
prompt2 = "New Study Reveals Health Benefits of Sleep (Generated Sample 2)"
print(f"\nPrompt: {prompt2}")
generated_text2 = generator(
    prompt2,
    max_length=120,
    num_return_sequences=1,
    pad_token_id=tokenizer.eos_token_id,
    do_sample=True,
    top_k=50,
    temperature=0.8
)[0]['generated_text']
print(f"Generated Text 2:\n{generated_text2}")
print("-" * 30)

# Example 3: A prompt that requires the model to 'continue' a sentence
prompt3 = "The Amazon River, located in South America,"
print(f"\nPrompt: {prompt3}")
generated_text3 = generator(
    prompt3,
    max_length=80,
    num_return_sequences=1,
    pad_token_id=tokenizer.eos_token_id,
    do_sample=True,
    top_k=50,
    temperature=0.9
)[0]['generated_text']
print(f"Generated Text 3:\n{generated_text3}")
print("-" * 30)

print("\nText generation process complete.")
print("\nTips for improving generation quality:")
print("- Experiment with `max_length`, `num_return_sequences`, `temperature`, `top_k`, `top_p` parameters.")
print("- For best results, the prompt should ideally end with the same separator used during training (if any).")
print("- You might need to post-process the generated text to remove the input prompt or any trailing special tokens like `EOS`.")

